👋 Bonjour, je suis Minh-Hien,
Data Scientist basé(e) à Bruxelles, Belgique.

J’ai une formation en technologies de l'information et en développement de systèmes d'information (licence et master), ainsi qu’une expérience dans le domaine de l’éducation. J’ai commencé ma carrière comme assistant(e) TIC et développeur(euse) web au sein du département des affaires académiques, puis comme assistant(e) technique pour la revue scientifique de l’université (@Scientific Support Unit), et enfin comme responsable administratif d’un programme post-universitaire (@Graduate School) dans une université multidisciplinaire au Vietnam.

J’ai acquis une solide expérience dans la gestion des données des étudiants de licence et de master, ainsi que dans le développement de systèmes de gestion universitaire. J’ai eu la chance de réaliser mon rêve d’étudier à l’étranger en obtenant une bourse pour un master aux Pays-Bas, suivi d’un doctorat en Belgique.

Après avoir obtenu mon doctorat en 2020, je me suis réorienté(e) vers le domaine de l’intelligence artificielle, en commençant par un apprentissage autodidacte de Python et R. Ma passion grandissante pour l’IA m’a poussé(e) à suivre une formation plus structurée et immersive. En mai 2021, j’ai été sélectionné(e) pour rejoindre le bootcamp intensif en temps plein AI & Machine Learning Operator organisé par BeCode.org à Gand, en Belgique.

Pendant cette formation, j’ai travaillé sur plusieurs projets pratiques portant à la fois sur le machine learning et le deep learning. Ces projets m’ont permis de renforcer mes compétences techniques tout en développant mes aptitudes en résolution de problèmes, en travail d’équipe et en communication. J’ai également acquis une expérience concrète dans des environnements Agile, dans le développement de MVPs et dans la présentation professionnelle des résultats à des clients réels.

La réussite de ce bootcamp en décembre 2021 a marqué une étape clé dans ma transition vers le domaine de l’IA, me dotant à la fois des compétences techniques et transversales nécessaires pour contribuer efficacement à des projets axés sur les données.

Je suis actuellement Data Scientist chez Moody’s Analytics (depuis 2021), spécialisé(e) dans l’analyse de données financières, l’automatisation des processus, et l’optimisation basée sur l’IA. J’ai dirigé plusieurs projets à fort impact, notamment l’analyse de tendances boursières, la réconciliation de données financières, et l’automatisation de processus, permettant une prise de décision basée sur les données pour des entreprises cotées en bourse.

🔧 Technologies préférées :
#python, #pandas, #matplotlib, #scikit-learn, #plotly, #tensorflow, #NLP, #computer vision

💬 Domaines de discussion :
Traitement et transformation de données

Construction et optimisation de modèles de machine learning

Techniques de traitement du langage naturel (NLP)

Déploiement de modèles en production

📧 Contact : andyvo.bxl@gmail.com
📎 LinkedIn | Facebook

🔍 Quelques projets marquants réalisés lors du bootcamp BeCode :
1. ImmoEliza Analysis
Objectif : aider une entreprise immobilière belge à prédire les prix de vente des biens via un modèle de machine learning prenant en compte des variables comme la localisation, la surface ou le type de bien.

2. Accidents de la route à New York (NYC Motor Vehicle Crashes)
Nettoyage d’un jeu de données complexe sur les accidents de la circulation à NYC. Création d’un modèle pour prédire les rues les plus dangereuses à partir des données de la police (NYPD).

3. Système automatisé de test de roulements (bearing testing)
Développement d’un modèle de maintenance prédictive à partir de données recueillies par une machine de test. Objectif : détecter les roulements défectueux pour éviter les pannes.

4. Prédiction de la qualité des vins (Wine Quality Prediction)
Création d’un modèle de deep learning avec Keras pour prédire la qualité d’un vin (bon ou mauvais), en optimisant la performance via le tuning d’hyperparamètres.

5. Analyse de sentiments (NLP) – Réactions sur les réseaux sociaux
Réalisation d’une analyse des réactions des utilisateurs Twitter à une série Netflix (#queengambit ou autre). Scraping de 10 000 tweets, modélisation de l’analyse de sentiments, et développement d’une application de visualisation.

6. Détection de grains de beauté – Projet de vision par ordinateur
Application de CNN (réseaux de neurones convolutifs) pour classer des images médicales et détecter des grains de beauté suspects.

7. Utilisation de Snorkel pour détecter les modifications de CCT (conventions collectives de travail)
Projet NLP visant à automatiser l’analyse de conventions collectives pour extraire les changements clés, avec structuration des informations pour intégration dans des plateformes de reporting.
